{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyP2JeIyoEAp1NY+X1e7PzRp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosinator/cil-sentiment/blob/main/infer_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y4YS6EB03L-4"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip3 install transformers emoji==0.6.0 keras_nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RjLy6xA3NDp",
        "outputId": "17615a85-6be0-4cc0-abc9-853868ee851d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 36.9 MB/s eta 0:00:00\n",
            "Collecting emoji==0.6.0\n",
            "  Downloading emoji-0.6.0.tar.gz (51 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.0/51.0 kB 6.0 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.6.0-py3-none-any.whl (576 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 576.5/576.5 kB 38.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 29.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 78.2 MB/s eta 0:00:00\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 72.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Collecting keras-core (from keras_nlp)\n",
            "  Downloading keras_core-0.1.2-py3-none-any.whl (753 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 753.1/753.1 kB 63.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.4.2)\n",
            "Collecting tensorflow-text (from keras_nlp)\n",
            "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 51.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Collecting namex (from keras-core->keras_nlp)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.8.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (0.1.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.14.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (0.14.0)\n",
            "Collecting tensorflow<2.14,>=2.13.0 (from tensorflow-text->keras_nlp)\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 524.1/524.1 MB 2.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.56.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 53.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 121.3 MB/s eta 0:00:00\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 440.8/440.8 kB 40.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.3.0)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (3.2.2)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py): started\n",
            "  Building wheel for emoji (setup.py): finished with status 'done'\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49720 sha256=c7d264e5bf541eb1f0ae0103fc223d6206a59dcb0849117890ff911530ad2b10\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/bd/d9/310c33c45a553798a714e27e3b8395d37128425442b8c78e07\n",
            "Successfully built emoji\n",
            "Installing collected packages: tokenizers, safetensors, namex, emoji, typing-extensions, tensorflow-estimator, keras, huggingface-hub, transformers, keras-core, tensorboard, tensorflow, tensorflow-text, keras_nlp\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed emoji-0.6.0 huggingface-hub-0.16.4 keras-2.13.1 keras-core-0.1.2 keras_nlp-0.6.0 namex-0.0.7 safetensors-0.3.1 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-text-2.13.0 tokenizers-0.13.3 transformers-4.31.0 typing-extensions-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/carlosinator/cil-sentiment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDGkh6hX3US9",
        "outputId": "4b5e647e-f6b3-4739-cced-c7a0654773a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cil-sentiment'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 119 (delta 58), reused 17 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (119/119), 162.02 KiB | 9.53 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, TFAutoModel, AutoConfig, TFAutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import re\n",
        "import subprocess as sp\n",
        "import os\n",
        "from threading import Thread , Timer\n",
        "import sched, time\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"./cil-sentiment/models\")\n",
        "sys.path.append(\"./cil-sentiment/\")\n",
        "from gru_models import GRUModel, VGRUModel\n",
        "import utils\n",
        "\n",
        "# reproducibility\n",
        "transformers.set_seed(0) # sets the seed in random, numpy, and tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qctsllC3UvU",
        "outputId": "636121a2-af24-43d6-9b01-266b43e3c0ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy Test Data"
      ],
      "metadata": {
        "id": "fvLKkJgg5yD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy preprocessed data from google cloud storage\n",
        "!gsutil cp \"gs://cil_2023/test_preprocessed.txt\" ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaHY7C_J3Wi-",
        "outputId": "26404cfe-1961-42f6-804b-981b1a1ac548"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cil_2023/test_preprocessed.txt...\n",
            "/ [1 files][798.3 KiB/798.3 KiB]                                                \n",
            "Operation completed over 1 objects/798.3 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse Test Data into df"
      ],
      "metadata": {
        "id": "crkWeYSf5v4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file line by line, split on the first comma only, and append to a list\n",
        "data = []\n",
        "with open('test_preprocessed.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        split_line = line.strip().split(',', 1)  # split only on the first comma\n",
        "        if len(split_line) == 2:  # to ensure there are no lines with missing values\n",
        "            data.append(split_line)\n",
        "\n",
        "# Create a DataFrame from the list\n",
        "df = pd.DataFrame(data, columns=['id', 'tweet'])"
      ],
      "metadata": {
        "id": "vVqjcc6s4wGU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize Dataset"
      ],
      "metadata": {
        "id": "t5u-78mk5s2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize data set\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenized_tweets = tokenizer.batch_encode_plus(df['tweet'].tolist(),\n",
        "                                    padding=\"max_length\", max_length=73, truncation=True,\n",
        "                                    return_tensors='tf')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjUyAK3V5j_8",
        "outputId": "97ef1a57-c0df-4e93-b031-084cf4505229"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some variable definitions\n",
        "USE_MODEL = \"read\"\n",
        "run_name = \"inference_\" + USE_MODEL + \"_fullmodel\""
      ],
      "metadata": {
        "id": "EOMPawxX9VDF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the model\n",
        "!gsutil cp -r {\"gs://cil_2023/models/\" + run_name} ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZB8XZUg8tvl",
        "outputId": "3c54a074-bf5f-47cb-85d2-9abc76997260"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cil_2023/models/inference_read_fullmodel/fingerprint.pb...\n",
            "Copying gs://cil_2023/models/inference_read_fullmodel/keras_metadata.pb...\n",
            "Copying gs://cil_2023/models/inference_read_fullmodel/saved_model.pb...\n",
            "Copying gs://cil_2023/models/inference_read_fullmodel/variables/variables.data-00000-of-00001...\n",
            "==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "- [4 files][527.7 MiB/527.7 MiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://cil_2023/models/inference_read_fullmodel/variables/variables.index...\n",
            "- [5 files][527.7 MiB/527.7 MiB]                                                \n",
            "Operation completed over 5 objects/527.7 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = tf.keras.models.load_model(run_name)"
      ],
      "metadata": {
        "id": "e5-cNcWJHNCA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the inference\n",
        "probabilities = model.predict(dict(tokenized_tweets))"
      ],
      "metadata": {
        "id": "I81mjcajJ6P2",
        "outputId": "34138503-d443-4509-ab3f-66ea4364882a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 33s 86ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_MODEL == \"basemodel\":\n",
        "  # on base model, we will get logits, so apply a softmax\n",
        "  probabilities = tf.nn.softmax(probabilities[\"logits\"])\n",
        "\n",
        "# Convert probabilities into class predictions (0 or 1)\n",
        "# and then map 0 to 1 and 1 to -1 (that is Leo's great mapping:))\n",
        "predictions = np.argmax(probabilities, axis=-1)\n",
        "predictions = np.where(predictions == 0, 1, -1)\n",
        "\n",
        "# Write the results to a text file\n",
        "with open('predictions.csv', 'w') as file:\n",
        "    file.write(\"Id,Prediction\\n\")\n",
        "    for i, prediction in enumerate(predictions):\n",
        "        file.write(f\"{df['id'].iloc[i]},{prediction}\\n\")"
      ],
      "metadata": {
        "id": "e0Ku9oIp6ILD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "SgedNopyJf5n",
        "outputId": "38f6afac-ecc3-4262-fee6-f02e6a206dc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"gru_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " tf_roberta_model (TFRobert  multiple                  134899968 \n",
            " aModel)                                                         \n",
            "                                                                 \n",
            " layer_normalization (Layer  multiple                  32        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  12304     \n",
            "                                                                 \n",
            " bidirectional (Bidirection  multiple                  1248      \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134913586 (514.65 MB)\n",
            "Trainable params: 13618 (53.20 KB)\n",
            "Non-trainable params: 134899968 (514.60 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UNnIOcerNAR3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}