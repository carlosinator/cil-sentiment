{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosinator/cil-sentiment/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Setup"
      ],
      "metadata": {
        "id": "XONmweYhecr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "4G2RQFYQkp77"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip3 install transformers emoji==0.6.0 keras_nlp"
      ],
      "metadata": {
        "id": "4kj6bSoKelL7",
        "outputId": "e0095a51-c375-4f49-e0af-3d8bccb1bc40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: emoji==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: keras_nlp in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.4.2)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2.13.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.8.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (0.1.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.14.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (0.14.0)\n",
            "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (2.13.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.56.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text->keras_nlp) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/carlosinator/cil-sentiment.git"
      ],
      "metadata": {
        "id": "UQuSJ6uggr1q",
        "outputId": "370f82da-cda9-47c7-c478-105edfb5b72c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cil-sentiment' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, TFAutoModel, AutoConfig, TFAutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import re\n",
        "import subprocess as sp\n",
        "import os\n",
        "from threading import Thread , Timer\n",
        "import sched, time\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"./cil-sentiment/models\")\n",
        "sys.path.append(\"./cil-sentiment/\")\n",
        "from gru_models import GRUModel, VGRUModel\n",
        "import utils\n",
        "\n",
        "# reproducibility\n",
        "transformers.set_seed(0) # sets the seed in random, numpy, and tf"
      ],
      "metadata": {
        "id": "aUf_eJZ8esrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5805f912-971d-45e3-cd60-7a5fc5d84497"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp \"gs://cil_2023/train_pos_full_preprocessed_without_duplicates.txt\" .\n",
        "!gsutil cp \"gs://cil_2023/train_neg_full_preprocessed_without_duplicates.txt\" .\n",
        "\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "filename_train_pos = \"train_pos_full_preprocessed_without_duplicates.txt\"\n",
        "filename_train_neg = \"train_neg_full_preprocessed_without_duplicates.txt\"\n",
        "\n",
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "# tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "dDHTZfUcezks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6bb7423-241d-4a25-cc04-15868d627aed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cil_2023/train_pos_full_preprocessed_without_duplicates.txt...\n",
            "| [1 files][ 74.6 MiB/ 74.6 MiB]                                                \n",
            "Operation completed over 1 objects/74.6 MiB.                                     \n",
            "Copying gs://cil_2023/train_neg_full_preprocessed_without_duplicates.txt...\n",
            "/ [1 files][ 91.6 MiB/ 91.6 MiB]                                                \n",
            "Operation completed over 1 objects/91.6 MiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset\n",
        "dataset_pos_pd = pd.read_table(filename_train_pos, sep='\\r\\n', header=None, names=['text'])\n",
        "dataset_neg_pd = pd.read_table(filename_train_neg, sep='\\r\\n', header=None, names=['text'])\n",
        "dataset_pos_pd['label'] = 0\n",
        "dataset_neg_pd['label'] = 1\n",
        "dataset_pd = pd.concat([dataset_pos_pd, dataset_neg_pd])\n",
        "\n",
        "# shuffle\n",
        "dataset_pd = dataset_pd.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "# tokenize data set\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "texts = tokenizer.batch_encode_plus(dataset_pd['text'].tolist(),\n",
        "                                    padding=True, truncation=True,\n",
        "                                    return_tensors='tf')\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((dict(texts), dataset_pd['label']))\n",
        "\n",
        "# split training / validation\n",
        "batch_size = 256 # * tpu_strategy.num_replicas_in_sync\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "val_data_size = int(0.1 * len(dataset_pd.index))\n",
        "test_data_size = int(0.1 * len(dataset_pd.index))\n",
        "train_data_size = len(dataset_pd.index) - val_data_size - test_data_size\n",
        "val_ds = dataset.take(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "test_ds = dataset.take(test_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = dataset.skip(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "-qI7H3nSfFg5",
        "outputId": "37c69d0b-39e0-409c-bce7-3a8842027b4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a25b9d79ca72>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  dataset_pos_pd = pd.read_table(filename_train_pos, sep='\\r\\n', header=None, names=['text'])\n",
            "<ipython-input-6-a25b9d79ca72>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  dataset_neg_pd = pd.read_table(filename_train_neg, sep='\\r\\n', header=None, names=['text'])\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model for inference"
      ],
      "metadata": {
        "id": "OWX7MndbsU3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess as sp\n",
        "import os\n",
        "from threading import Thread , Timer\n",
        "import sched, time\n",
        "\n",
        "gpu_hist = {\n",
        "    \"mib\" : [],\n",
        "    \"percent\" : [],\n",
        "    \"counter\" : 0,\n",
        "}\n",
        "\n",
        "def get_gpu_memory():\n",
        "\n",
        "    out = !nvidia-smi --query-gpu=memory.used,utilization.gpu --format=csv\n",
        "\n",
        "    float_pattern = r'\\d+\\.\\d+|\\d+'\n",
        "    numbers = re.findall(float_pattern, out[1])\n",
        "\n",
        "    # Extract the MiB value and the percentage\n",
        "    mib_val = float(numbers[0])\n",
        "    perc = float(numbers[1])\n",
        "\n",
        "    gpu_hist[\"counter\"] += 1\n",
        "\n",
        "    # print(mib_val, perc)\n",
        "\n",
        "    gpu_hist[\"mib\"].append(mib_val)\n",
        "    gpu_hist[\"percent\"].append(perc)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def track_gpu_mem(interval=5.0, dictname=\"gpu_hist.pkl\"):\n",
        "    \"\"\"\n",
        "        This function calls itself every 5 secs and print the gpu_memory.\n",
        "    \"\"\"\n",
        "    thd = Timer(interval, track_gpu_mem, [interval, dictname])\n",
        "    thd.start()\n",
        "    get_gpu_memory()\n",
        "\n",
        "    gpu_hist[\"interval\"] = interval\n",
        "\n",
        "    with open(dictname, 'wb') as f:\n",
        "      pickle.dump(gpu_hist, f)\n",
        "\n",
        "    return thd"
      ],
      "metadata": {
        "id": "1cTLXy2MLjKa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_MODEL = \"basemodel\"\n",
        "LEARNING_RATE = 2e-5\n",
        "EPOCHS = 10\n",
        "model = utils.get_model(model_name, LEARNING_RATE, USE_MODEL)\n",
        "history = {}\n",
        "\n",
        "run_name = \"inference_\" + USE_MODEL + \"_fullmodel\"\n",
        "hd_name = run_name + \"_dict.pkl\"\n",
        "gpu_hist_name = \"gpu_hist_\" + USE_MODEL + \".pkl\""
      ],
      "metadata": {
        "id": "42ATZvFpsVTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c4ae7a-aa6c-4d87-b499-085865ea83fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "O3dMXr2gu4QX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(patience=3) # early stopping def\n",
        "gpu_mem_proc = track_gpu_mem(10.0, dictname=gpu_hist_name) # initialize gpu tracking\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    callbacks=[callback]\n",
        "    )\n",
        "\n",
        "gpu_mem_proc.join()"
      ],
      "metadata": {
        "id": "R5o95XE4yQRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5321a1-7f2d-4c6d-e588-7ca720ae875a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7981/7981 [==============================] - 3007s 371ms/step - loss: 0.2373 - sparse_categorical_accuracy: 0.8998 - val_loss: 0.2188 - val_sparse_categorical_accuracy: 0.9087\n",
            "Epoch 2/10\n",
            "3271/7981 [===========>..................] - ETA: 27:37 - loss: 0.2061 - sparse_categorical_accuracy: 0.9143"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7981/7981 [==============================] - 2906s 364ms/step - loss: 0.2016 - sparse_categorical_accuracy: 0.9165 - val_loss: 0.2237 - val_sparse_categorical_accuracy: 0.9095\n",
            "Epoch 3/10\n",
            "7981/7981 [==============================] - 2907s 364ms/step - loss: 0.1742 - sparse_categorical_accuracy: 0.9292 - val_loss: 0.2346 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 4/10\n",
            "7981/7981 [==============================] - 2906s 364ms/step - loss: 0.1492 - sparse_categorical_accuracy: 0.9406 - val_loss: 0.2540 - val_sparse_categorical_accuracy: 0.9056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "model.save(run_name)\n",
        "# save training history\n",
        "with open(hd_name, 'wb') as f:\n",
        "  pickle.dump(history, f)\n",
        "\n",
        "#Note: the gpu history should already been stored at gpu_hist_name\n",
        "\n",
        "!gsutil cp -r {run_name + \"/\"} \"gs://cil_2023/models/\"\n",
        "!gsutil cp {hd_name} \"gs://cil_2023/models/\"\n",
        "!gsutil cp {gpu_hist_name} \"gs://cil_2023/models/\""
      ],
      "metadata": {
        "id": "9gEyCl3TywTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7089f87e-8e94-43e0-f044-1bc3924c6701"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://inference_basemodel_fullmodel/fingerprint.pb [Content-Type=application/octet-stream]...\n",
            "Copying file://inference_basemodel_fullmodel/keras_metadata.pb [Content-Type=application/octet-stream]...\n",
            "Copying file://inference_basemodel_fullmodel/saved_model.pb [Content-Type=application/octet-stream]...\n",
            "Copying file://inference_basemodel_fullmodel/variables/variables.index [Content-Type=application/octet-stream]...\n",
            "\\\n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying file://inference_basemodel_fullmodel/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "- [5 files][  1.5 GiB/  1.5 GiB]   23.1 MiB/s                                   \n",
            "Operation completed over 5 objects/1.5 GiB.                                      \n",
            "Copying file://inference_basemodel_fullmodel_dict.pkl [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "-\n",
            "Operation completed over 1 objects/1.5 GiB.                                      \n",
            "Copying file://gpu_hist_basemodel.pkl [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/88.0 B.                                       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve model for inference"
      ],
      "metadata": {
        "id": "Y7aidOPepByz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment if trained_model is not on local machine\n",
        "# !gsutil cp -r {\"gs://cil_2023/models/\" + run_name} .\n",
        "trained_model = tf.keras.models.load_model(run_name)\n",
        "trained_model.summary()"
      ],
      "metadata": {
        "id": "1w1dWwmnpGHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cbc08f1-f9e0-43a4-b12d-5e0533b628db"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_roberta_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " roberta (TFRobertaMainLaye  multiple                  134309376 \n",
            " r)                                                              \n",
            "                                                                 \n",
            " classifier (TFRobertaClass  multiple                  592130    \n",
            " ificationHead)                                                  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134901506 (514.61 MB)\n",
            "Trainable params: 134901506 (514.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test accuracy"
      ],
      "metadata": {
        "id": "D71Ow9UTsNnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trained_model.predict(test_ds)\n",
        "\n",
        "if USE_MODEL == \"basemodel\":\n",
        "  preds = tf.keras.layers.Softmax()(preds)"
      ],
      "metadata": {
        "id": "TVJwoShAsP2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ST0CbuP3HyTG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}