{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosinator/cil-sentiment/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfJ1Y6JLrUoW"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip3 install wordsegment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq3DhVJRr0ai"
      },
      "outputs": [],
      "source": [
        "# colab auth\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wZh5F8dKrYK"
      },
      "outputs": [],
      "source": [
        "# copy data from google cloud storage\n",
        "!gsutil cp \"gs://cil_2023/train_pos_full.txt\" .\n",
        "!gsutil cp \"gs://cil_2023/train_neg_full.txt\" .\n",
        "!gsutil cp \"gs://cil_2023/test_data.txt\" ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPSJ0A1-jUyV"
      },
      "outputs": [],
      "source": [
        "# general imports\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from wordsegment import load, segment\n",
        "load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBP_a4mijUyW"
      },
      "outputs": [],
      "source": [
        "# base paths (overwrite maybe for local use or colab use)\n",
        "BASE_PATH = Path()\n",
        "BASE_OUT_PATH = Path()\n",
        "\n",
        "# BASE_PATH = Path(\"./twitter-dataset/\")\n",
        "BASE_OUT_PATH = Path() / \"prepro_output\"\n",
        "\n",
        "# ensure output path exists\n",
        "BASE_OUT_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# define paths to raw data\n",
        "TRAIN_POS_PATH = BASE_PATH / \"train_pos_full.txt\"\n",
        "TRAIN_NEG_PATH = BASE_PATH / \"train_neg_full.txt\"\n",
        "TEST_PATH = BASE_PATH / \"test_data.txt\"\n",
        "\n",
        "# define paths to output data\n",
        "TRAIN_POS_OUT_PATH = BASE_OUT_PATH / \"train_pos_full_preprocessed_without_duplicates.txt\"\n",
        "TRAIN_NEG_OUT_PATH = BASE_OUT_PATH / \"train_neg_full_preprocessed_without_duplicates.txt\"\n",
        "TEST_OUT_PATH = BASE_OUT_PATH / \"test_data_preprocessed_without_duplicates.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRXs8nc2Kxs4"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "tweets_train_pos = pd.read_fwf(TRAIN_POS_PATH, sep='\\n', header=None)[0].tolist()\n",
        "tweets_train_neg = pd.read_fwf(TRAIN_NEG_PATH, sep='\\n', header=None)[0].tolist()\n",
        "tweets_test = pd.read_fwf(TEST_PATH, sep='\\n', header=None)[0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkhxcI6vjUyW"
      },
      "outputs": [],
      "source": [
        "# remove duplicate strings from list and print out how many were removed\n",
        "def remove_duplicates(tweets):\n",
        "    print(\"------------ REMOVING DUPLICATES ------------\")\n",
        "    print(\"before removing duplicates: \", len(tweets))\n",
        "    tweets = list(set(tweets))\n",
        "    print(\"after removing duplicates: \", len(tweets))\n",
        "    return tweets\n",
        "\n",
        "\n",
        "# remove duplicates\n",
        "tweets_train_pos = remove_duplicates(tweets_train_pos)\n",
        "tweets_train_neg = remove_duplicates(tweets_train_neg)\n",
        "tweets_test = remove_duplicates(tweets_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IGk8uGNv75J"
      },
      "outputs": [],
      "source": [
        "def unhashtag(tweet):\n",
        "  offset = 0\n",
        "  for h in re.finditer(r'#\\S+', tweet):\n",
        "    start, end = h.span()\n",
        "    new_string = ' '.join(segment(h.group(0))) + ' '\n",
        "    tweet = tweet[:(start+offset)] + new_string + tweet[(end+offset+1):]\n",
        "    offset += len(new_string) - (end - start + 1)\n",
        "  return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT1uB561v_D0"
      },
      "outputs": [],
      "source": [
        "unhashtag(\"bla bla #ihatethis bla bla\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ3gmZ1KxM25"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "\n",
        "# should take about 5 min\n",
        "for tweetlist in [tweets_train_pos, tweets_train_neg, tweets_test]:\n",
        "  for i, tweet in enumerate(tweetlist):\n",
        "    if '#' in tweet:\n",
        "      tweetlist[i] = unhashtag(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIXvecilNlL7"
      },
      "outputs": [],
      "source": [
        "# write preprocessed tweets back to file\n",
        "for filename, tweetlist in [(TRAIN_POS_OUT_PATH, tweets_train_pos),\n",
        "                            (TRAIN_NEG_OUT_PATH, tweets_train_neg),\n",
        "                            (TEST_OUT_PATH, tweets_test)]:\n",
        "  with open(filename, 'w') as f:\n",
        "    for line in tweetlist:\n",
        "      f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVdxc0slSR8g"
      },
      "outputs": [],
      "source": [
        "# (optional) copy data to google cloud storage\n",
        "!gsutil cp $TRAIN_POS_OUT_PATH \"gs://cil_2023/\"\n",
        "!gsutil cp $TRAIN_NEG_OUT_PATH \"gs://cil_2023/\"\n",
        "!gsutil cp $TEST_OUT_PATH \"gs://cil_2023/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29rZEhZlmJUH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}