{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPRoL7XvuZUJCMmRnsnOIQ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosinator/cil-sentiment/blob/main/inference_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Setup"
      ],
      "metadata": {
        "id": "XONmweYhecr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "4G2RQFYQkp77"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip3 install transformers emoji==0.6.0 keras_nlp"
      ],
      "metadata": {
        "id": "4kj6bSoKelL7",
        "outputId": "753d2ecc-4bff-404d-effa-5fe0b9d36ffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: emoji==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: keras_nlp in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.4.2)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2.13.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (2.13.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.14.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.56.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-core->keras_nlp) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/carlosinator/cil-sentiment.git"
      ],
      "metadata": {
        "id": "UQuSJ6uggr1q",
        "outputId": "006d8574-db43-4c3e-fa27-81ddcb1837ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cil-sentiment' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, TFAutoModel, AutoConfig, TFAutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import re\n",
        "import subprocess as sp\n",
        "import os\n",
        "from threading import Thread , Timer\n",
        "import sched, time\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"./cil-sentiment/models\")\n",
        "sys.path.append(\"./cil-sentiment/\")\n",
        "from gru_models import GRUModel, VGRUModel\n",
        "import utils\n",
        "\n",
        "# reproducibility\n",
        "transformers.set_seed(0) # sets the seed in random, numpy, and tf"
      ],
      "metadata": {
        "id": "aUf_eJZ8esrm",
        "outputId": "c8ddb376-33aa-4ca7-ca5d-d6ed2c848620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp \"gs://cil_2023/train_pos_full_preprocessed_without_duplicates.txt\" .\n",
        "!gsutil cp \"gs://cil_2023/train_neg_full_preprocessed_without_duplicates.txt\" .\n",
        "\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "filename_train_pos = \"train_pos_full_preprocessed_without_duplicates.txt\"\n",
        "filename_train_neg = \"train_neg_full_preprocessed_without_duplicates.txt\"\n",
        "\n",
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "# tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "dDHTZfUcezks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0eee34-85ae-4f65-f0fd-e47f7873a09e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cil_2023/train_pos_full_preprocessed_without_duplicates.txt...\n",
            "\\ [1 files][ 74.6 MiB/ 74.6 MiB]                                                \n",
            "Operation completed over 1 objects/74.6 MiB.                                     \n",
            "Copying gs://cil_2023/train_neg_full_preprocessed_without_duplicates.txt...\n",
            "| [1 files][ 91.6 MiB/ 91.6 MiB]                                                \n",
            "Operation completed over 1 objects/91.6 MiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset\n",
        "dataset_pos_pd = pd.read_table(filename_train_pos, sep='\\r\\n', header=None, names=['text'])\n",
        "dataset_neg_pd = pd.read_table(filename_train_neg, sep='\\r\\n', header=None, names=['text'])\n",
        "dataset_pos_pd['label'] = 0\n",
        "dataset_neg_pd['label'] = 1\n",
        "dataset_pd = pd.concat([dataset_pos_pd, dataset_neg_pd])\n",
        "\n",
        "# shuffle\n",
        "dataset_pd = dataset_pd.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "# tokenize data set\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "texts = tokenizer.batch_encode_plus(dataset_pd['text'].tolist(),\n",
        "                                    padding=True, truncation=True,\n",
        "                                    return_tensors='tf')\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((dict(texts), dataset_pd['label']))\n",
        "\n",
        "# split training / validation\n",
        "batch_size = 1024 # * tpu_strategy.num_replicas_in_sync\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "val_data_size = int(0.1 * len(dataset_pd.index))\n",
        "test_data_size = int(0.1 * len(dataset_pd.index))\n",
        "train_data_size = len(dataset_pd.index) - val_data_size - test_data_size\n",
        "val_ds = dataset.take(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "test_ds = dataset.take(test_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = dataset.skip(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "-qI7H3nSfFg5",
        "outputId": "5ae6d2b8-6b70-41e3-d413-b222c439a441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-364751df9cd3>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  dataset_pos_pd = pd.read_table(filename_train_pos, sep='\\r\\n', header=None, names=['text'])\n",
            "<ipython-input-6-364751df9cd3>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  dataset_neg_pd = pd.read_table(filename_train_neg, sep='\\r\\n', header=None, names=['text'])\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model for inference"
      ],
      "metadata": {
        "id": "OWX7MndbsU3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess as sp\n",
        "import os\n",
        "from threading import Thread , Timer\n",
        "import sched, time\n",
        "\n",
        "gpu_hist = {\n",
        "    \"mib\" : [],\n",
        "    \"percent\" : [],\n",
        "    \"counter\" : 0,\n",
        "}\n",
        "\n",
        "def get_gpu_memory():\n",
        "\n",
        "    out = !nvidia-smi --query-gpu=memory.used,utilization.gpu --format=csv\n",
        "\n",
        "    float_pattern = r'\\d+\\.\\d+|\\d+'\n",
        "    numbers = re.findall(float_pattern, out[1])\n",
        "\n",
        "    # Extract the MiB value and the percentage\n",
        "    mib_val = float(numbers[0])\n",
        "    perc = float(numbers[1])\n",
        "\n",
        "    gpu_hist[\"counter\"] += 1\n",
        "\n",
        "    # print(mib_val, perc)\n",
        "\n",
        "    gpu_hist[\"mib\"].append(mib_val)\n",
        "    gpu_hist[\"percent\"].append(perc)\n",
        "\n",
        "    with open(\"gpu_hist.pkl\", 'wb') as f:\n",
        "      pickle.dump(gpu_hist, f)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def track_gpu_mem(interval=5.0):\n",
        "    \"\"\"\n",
        "        This function calls itself every 5 secs and print the gpu_memory.\n",
        "    \"\"\"\n",
        "    thd = Timer(interval, track_gpu_mem)\n",
        "    thd.start()\n",
        "    get_gpu_memory()\n",
        "\n",
        "    gpu_hist[\"interval\"] = interval\n",
        "\n",
        "    return thd"
      ],
      "metadata": {
        "id": "1cTLXy2MLjKa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RxdCPGLvX-rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_MODEL = \"read\"\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 1\n",
        "model = utils.get_model(model_name, LEARNING_RATE, USE_MODEL)"
      ],
      "metadata": {
        "id": "42ATZvFpsVTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(patience=3) # early stopping def\n",
        "gpu_mem_proc = track_gpu_mem(10.0) # initialize gpu tracking\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    callbacks=[callback]\n",
        "    )\n",
        "\n",
        "run_name = \"inference_\" + USE_MODEL + \"_lr=\" + f\"{LEARNING_RATE:.2e}\" + \"_fullmodel\"\n",
        "hd_name = run_name + \"_dict.pkl\"\n",
        "\n",
        "model.save(run_name)\n",
        "with open(hd_name, 'wb') as f:\n",
        "    pickle.dump(history, f)\n",
        "\n",
        "!gs cp {run_name} \"gs://cil_2023/models/\"\n",
        "!gs cp {hd_name} \"gs://cil_2023/models/\"\n",
        "\n",
        "gpu_mem_proc.join()"
      ],
      "metadata": {
        "id": "R5o95XE4yQRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_name = \"inference_\" + USE_MODEL + \"_lr=\" + f\"{LEARNING_RATE:.2e}\" + \"_fullmodel\"\n",
        "model.save(run_name)"
      ],
      "metadata": {
        "id": "3SEfrkyP3Y4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve model for inference"
      ],
      "metadata": {
        "id": "Y7aidOPepByz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_name = \"hps_basemodel_lr=1.00e-06\"\n",
        "!gsutil cp {\"gs://cil_2023/models/\" + test_name} ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIhEgGzk3WWk",
        "outputId": "f4436317-831b-49a5-f3f1-0b69b50ef613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cil_2023/models/hps_basemodel_lr=1.00e-06.h5...\n",
            "/ [0 files][    0.0 B/514.9 MiB]                                                \r==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "| [1 files][514.9 MiB/514.9 MiB]   28.8 MiB/s                                   \n",
            "Operation completed over 1 objects/514.9 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.built = True\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz4W61aQRhRC",
        "outputId": "1b523505-0290-4fbf-f947-7065f895de7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"gru_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " tf_roberta_model (TFRobert  multiple                  134899968 \n",
            " aModel)                                                         \n",
            "                                                                 \n",
            " layer_normalization (Layer  multiple                  0 (unused)\n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  0 (unused)\n",
            "                                                                 \n",
            " bidirectional (Bidirection  multiple                  0 (unused)\n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  0 (unused)\n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134899968 (514.60 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 134899968 (514.60 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = tf.keras.models.load_model('inference_read_lr=1.00e-03_fullmodel')\n",
        "# trained_model.load_weights(\"hps_basemodel_lr=1.00e-06.h5\")\n",
        "trained_model.summary()"
      ],
      "metadata": {
        "id": "1w1dWwmnpGHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model.built = True"
      ],
      "metadata": {
        "id": "UeQEKd0v_CUD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test accuracy"
      ],
      "metadata": {
        "id": "D71Ow9UTsNnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trained_model.predict(test_ds)\n",
        "\n",
        "if USE_MODEL == basemodel:\n",
        "  preds = tf.keras.layers.Softmax()(preds)"
      ],
      "metadata": {
        "id": "TVJwoShAsP2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST0CbuP3HyTG",
        "outputId": "b5b09eac-2983-4d9d-e5c0-273bbf8ab2a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.41746756 0.58253247]\n",
            " [0.39550942 0.6044906 ]\n",
            " [0.59442586 0.4055741 ]\n",
            " ...\n",
            " [0.35154378 0.6484562 ]\n",
            " [0.42560267 0.5743973 ]\n",
            " [0.2555229  0.7444771 ]]\n"
          ]
        }
      ]
    }
  ]
}