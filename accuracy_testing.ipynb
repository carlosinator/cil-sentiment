{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZgFEWuhCsefjR5AJEKv4p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosinator/cil-sentiment/blob/main/accuracy_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Setup"
      ],
      "metadata": {
        "id": "XONmweYhecr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "4G2RQFYQkp77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip3 install transformers emoji==0.6.0 keras_nlp"
      ],
      "metadata": {
        "id": "4kj6bSoKelL7",
        "outputId": "38b932c9-6338-4b36-decf-75c98cfdd495",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 49.5 MB/s eta 0:00:00\n",
            "Collecting emoji==0.6.0\n",
            "  Downloading emoji-0.6.0.tar.gz (51 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.0/51.0 kB 6.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.6.0-py3-none-any.whl (576 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 576.5/576.5 kB 50.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 30.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 107.2 MB/s eta 0:00:00\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 66.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Collecting keras-core (from keras_nlp)\n",
            "  Downloading keras_core-0.1.0-py3-none-any.whl (727 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 728.0/728.0 kB 55.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.4.2)\n",
            "Collecting tensorflow-text (from keras_nlp)\n",
            "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 115.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (2.12.0)\n",
            "Collecting namex (from keras-core->keras_nlp)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.14.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (0.14.0)\n",
            "Collecting tensorflow (from keras-core->keras_nlp)\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 524.1/524.1 MB 2.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.56.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow->keras-core->keras_nlp)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 37.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow->keras-core->keras_nlp)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 105.6 MB/s eta 0:00:00\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow->keras-core->keras_nlp)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 440.8/440.8 kB 44.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (2.3.0)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-core->keras_nlp) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (3.2.2)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py): started\n",
            "  Building wheel for emoji (setup.py): finished with status 'done'\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49720 sha256=c6f43c533bde5137d7c383d271aec5b6587d1725850d251b3e1578f9e5c63108\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/bd/d9/310c33c45a553798a714e27e3b8395d37128425442b8c78e07\n",
            "Successfully built emoji\n",
            "Installing collected packages: tokenizers, safetensors, namex, emoji, typing-extensions, tensorflow-estimator, keras, huggingface-hub, transformers, tensorboard, tensorflow, tensorflow-text, keras-core, keras_nlp\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed emoji-0.6.0 huggingface-hub-0.16.4 keras-2.13.1 keras-core-0.1.0 keras_nlp-0.6.0 namex-0.0.7 safetensors-0.3.1 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-text-2.13.0 tokenizers-0.13.3 transformers-4.30.2 typing-extensions-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/carlosinator/cil-sentiment.git"
      ],
      "metadata": {
        "id": "UQuSJ6uggr1q",
        "outputId": "b094cd7c-0101-4ff4-9bea-2f7be82ec086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cil-sentiment'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 86 (delta 36), reused 17 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (86/86), 119.78 KiB | 3.42 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, TFAutoModel, AutoConfig, TFAutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import re\n",
        "import subprocess as sp\n",
        "import os\n",
        "from threading import Thread , Timer\n",
        "import sched, time\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"./cil-sentiment/models\")\n",
        "sys.path.append(\"./cil-sentiment/\")\n",
        "from gru_models import GRUModel, VGRUModel\n",
        "import utils\n",
        "\n",
        "# reproducibility\n",
        "transformers.set_seed(0) # sets the seed in random, numpy, and tf"
      ],
      "metadata": {
        "id": "aUf_eJZ8esrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee01f224-8a22-427e-f727-09b3a812be08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp \"gs://cil_2023/train_pos_full_preprocessed_without_duplicates.txt\" .\n",
        "!gsutil cp \"gs://cil_2023/train_neg_full_preprocessed_without_duplicates.txt\" .\n",
        "\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "filename_train_pos = \"train_pos_full_preprocessed_without_duplicates.txt\"\n",
        "filename_train_neg = \"train_neg_full_preprocessed_without_duplicates.txt\"\n",
        "\n",
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "# tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "dDHTZfUcezks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30250b20-bccb-43ed-dd6b-2d477d4f8f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cil_2023/train_pos_full_preprocessed_without_duplicates.txt...\n",
            "\\ [1 files][ 74.6 MiB/ 74.6 MiB]                                                \n",
            "Operation completed over 1 objects/74.6 MiB.                                     \n",
            "Copying gs://cil_2023/train_neg_full_preprocessed_without_duplicates.txt...\n",
            "| [1 files][ 91.6 MiB/ 91.6 MiB]                                                \n",
            "Operation completed over 1 objects/91.6 MiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset\n",
        "dataset_pos_pd = pd.read_table(filename_train_pos, sep='\\r\\n', header=None, names=['text'])\n",
        "dataset_neg_pd = pd.read_table(filename_train_neg, sep='\\r\\n', header=None, names=['text'])\n",
        "dataset_pos_pd['label'] = 0\n",
        "dataset_neg_pd['label'] = 1\n",
        "dataset_pd = pd.concat([dataset_pos_pd, dataset_neg_pd])\n",
        "\n",
        "# shuffle\n",
        "dataset_pd = dataset_pd.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "# tokenize data set\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "texts = tokenizer.batch_encode_plus(dataset_pd['text'].tolist(),\n",
        "                                    padding=True, truncation=True,\n",
        "                                    return_tensors='tf')\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((dict(texts), dataset_pd['label']))\n",
        "\n",
        "# split training / validation\n",
        "batch_size = 1024 # * tpu_strategy.num_replicas_in_sync\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "val_data_size = int(0.1 * len(dataset_pd.index))\n",
        "test_data_size = int(0.1 * len(dataset_pd.index))\n",
        "train_data_size = len(dataset_pd.index) - val_data_size - test_data_size\n",
        "val_ds = dataset.take(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "test_ds = dataset.take(test_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = dataset.skip(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "-qI7H3nSfFg5",
        "outputId": "8beddfd5-4c7c-457e-fb83-91537d0bcdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "743aa62344874685973aa074d07ae86c",
            "953b35450ff94fe5b3be414e10957f41",
            "ee61a04ce0ee4bf3965a9cd80575e95a",
            "640e9ade4d584555888adfb7146fd318",
            "7b1dc9b4b2ea406da46825150b056d4e",
            "ac0c452e41c34570b02ef4e980304e4b",
            "042c22890e1c43f980c5af5afef52fa8",
            "a1c028f0558c46b990d0ae6bfae829cd",
            "0c79e25b6a8f440c90695a95ab6e7d94",
            "10725f6197d64ca98ddd1a2fa8b81ad1",
            "d3d5d417fbb44b908e9abd6d1f871063",
            "79458846df164f23a69bb8fe769f64e5",
            "951f3e547a384e3ebaaced9b477a45e4",
            "062f0d634a12414caffdfcf0ce1a5e52",
            "05fa52d6620d4804ab415625391e636a",
            "63fd80d006d043b898f661c6f500821c",
            "503dd2755d544e7f9c3274e48b56c6af",
            "f7c7d9b0c2b440a680360b4030ce71cf",
            "680efabce1124c458769e39993eebdac",
            "c393d79a166e4b3b986f7ae29ff85676",
            "b240eca4a5a6444185147124e10c5694",
            "6aee27f142134124a00a87a206d5ef7c",
            "9ed445bb2a004ef2a8022c7f67810913",
            "9333f9a51b264d86bb14af6eb6c8087e",
            "2d1dd868280349e191622b0cb4eef51b",
            "2dfa80c2219b4795bb276a819d7b816e",
            "fbe7c529a10b4a0daa3316fd4ed00639",
            "85e44f25e4d947d0b3e62f9dfd1458a7",
            "1d601a128f8c497fa9d56befd60c3bb6",
            "59ad84981209476f90e0eebdbf885849",
            "508fe595fd4d438a8a05b691857d2d7e",
            "7ccef76ce2474931a1dee982d7b04735",
            "7f6915c4d601438cbccf3d97d47695a7"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-364751df9cd3>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  dataset_pos_pd = pd.read_table(filename_train_pos, sep='\\r\\n', header=None, names=['text'])\n",
            "<ipython-input-6-364751df9cd3>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  dataset_neg_pd = pd.read_table(filename_train_neg, sep='\\r\\n', header=None, names=['text'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "743aa62344874685973aa074d07ae86c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79458846df164f23a69bb8fe769f64e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ed445bb2a004ef2a8022c7f67810913"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in Model"
      ],
      "metadata": {
        "id": "C6NbJNpbxkze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_MODEL = \"basemodel\" # alternatively 'read' or 'read-var'\n",
        "\n",
        "run_name = \"inference_\" + USE_MODEL + \"_fullmodel\"\n",
        "!gsutil cp {\"gs://cil_2023/models/\" + run_name} .\n",
        "\n",
        "model = tf.keras.models.load_model(run_name)"
      ],
      "metadata": {
        "id": "d8a3vdKhxmES"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}