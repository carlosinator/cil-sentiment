@inproceedings{nguyen2020bertweet,
  title={BERTweet: A pre-trained language model for English Tweets},
  author={Nguyen, Dat Quoc and Vu, Thanh and Nguyen, Anh Tuan},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={9--14},
  year={2020}
}

@article{bojanowski2017fasttext,
    author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
    title = "{Enriching Word Vectors with Subword Information}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {5},
    pages = {135-146},
    year = {2017},
    month = {06},
    abstract = "{Continuous word representations, trained on large unlabeled corpora are useful
                    for many natural language processing tasks. Popular models that learn such
                    representations ignore the morphology of words, by assigning a distinct vector
                    to each word. This is a limitation, especially for languages with large
                    vocabularies and many rare words. In this paper, we propose a new approach based
                    on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated
                    to each character n-gram; words being represented
                    as the sum of these representations. Our method is fast, allowing to train
                    models on large corpora quickly and allows us to compute word representations
                    for words that did not appear in the training data. We evaluate our word
                    representations on nine different languages, both on word similarity and analogy
                    tasks. By comparing to recently proposed morphological word representations, we
                    show that our vectors achieve state-of-the-art performance on these tasks.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00051},
    url = {https://doi.org/10.1162/tacl\_a\_00051},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00051/1567442/tacl\_a\_00051.pdf},
}


@misc{roberta,
  doi = {10.48550/ARXIV.1907.11692},
  url = {https://arxiv.org/abs/1907.11692},
  author = {Liu,  Yinhan and Ott,  Myle and Goyal,  Naman and Du,  Jingfei and Joshi,  Mandar and Chen,  Danqi and Levy,  Omer and Lewis,  Mike and Zettlemoyer,  Luke and Stoyanov,  Veselin},
  keywords = {Computation and Language (cs.CL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{read,
  doi = {10.48550/ARXIV.2305.15348},
  url = {https://arxiv.org/abs/2305.15348},
  author = {Wang,  Sid and Nguyen,  John and Li,  Ke and Wu,  Carole-Jean},
  keywords = {Machine Learning (cs.LG),  Artificial Intelligence (cs.AI),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {READ: Recurrent Adaptation of Large Transformers},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{abadi2016tensorflow,
      title={TensorFlow: A system for large-scale machine learning}, 
      author={Martín Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
      year={2016},
      eprint={1605.08695},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{wolf2020huggingfaces,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{devlin2018bert,
    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    year={2018},
    eprint={1810.04805},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{guo2017calibration,
      title={On Calibration of Modern Neural Networks}, 
      author={Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
      year={2017},
      eprint={1706.04599},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{vaicenavicius2019evaluating,
  title={Evaluating model calibration in classification},
  author={Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Sch{\"o}n, Thomas},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3459--3467},
  year={2019},
  organization={PMLR}
}

@misc{nixon2020measuring,
      title={Measuring Calibration in Deep Learning}, 
      author={Jeremy Nixon and Mike Dusenberry and Ghassen Jerfel and Timothy Nguyen and Jeremiah Liu and Linchuan Zhang and Dustin Tran},
      year={2020},
      eprint={1904.01685},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{segaran2009beautiful,
  title={Beautiful data: the stories behind elegant data solutions},
  author={Segaran, Toby and Hammerbacher, Jeff},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@misc{ethz-cil-text-classification-2023,
    author = {CIL Lecture},
    title = {ETHZ CIL Text Classification 2023},
    publisher = {Kaggle},
    year = {2023},
    url = {https://kaggle.com/competitions/ethz-cil-text-classification-2023}
}