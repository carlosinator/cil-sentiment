{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Search hyperparameters for LLM"
      ],
      "metadata": {
        "id": "XmWajEN6eT4T"
      },
      "id": "XmWajEN6eT4T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Setup"
      ],
      "metadata": {
        "id": "XONmweYhecr1"
      },
      "id": "XONmweYhecr1"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "4G2RQFYQkp77"
      },
      "id": "4G2RQFYQkp77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip3 install transformers emoji==0.6.0 keras_nlp"
      ],
      "metadata": {
        "id": "4kj6bSoKelL7"
      },
      "id": "4kj6bSoKelL7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/carlosinator/cil-sentiment.git"
      ],
      "metadata": {
        "id": "UQuSJ6uggr1q"
      },
      "id": "UQuSJ6uggr1q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, TFAutoModel, AutoConfig, TFAutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# reproducibility\n",
        "transformers.set_seed(0) # sets the seed in random, numpy, and tf"
      ],
      "metadata": {
        "id": "aUf_eJZ8esrm"
      },
      "id": "aUf_eJZ8esrm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./cil-sentiment/models\")\n",
        "from gru_models import GRUModel, VGRUModel"
      ],
      "metadata": {
        "id": "2DKmxZChk3F2"
      },
      "id": "2DKmxZChk3F2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp \"gs://cil_2023/train_pos_full_preprocessed_without_duplicates.txt\" .\n",
        "!gsutil cp \"gs://cil_2023/train_neg_full_preprocessed_without_duplicates.txt\" .\n",
        "\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "filename_train_pos = \"train_pos_full_preprocessed_without_duplicates.txt\"\n",
        "filename_train_neg = \"train_neg_full_preprocessed_without_duplicates.txt\"\n",
        "\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "dDHTZfUcezks"
      },
      "id": "dDHTZfUcezks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset\n",
        "dataset_pos_pd = pd.read_table(filename_train_pos, sep='\\r\\n', header=None, names=['text'])\n",
        "dataset_neg_pd = pd.read_table(filename_train_neg, sep='\\r\\n', header=None, names=['text'])\n",
        "dataset_pos_pd['label'] = 0\n",
        "dataset_neg_pd['label'] = 1\n",
        "dataset_pd = pd.concat([dataset_pos_pd, dataset_neg_pd])\n",
        "\n",
        "# shuffle\n",
        "dataset_pd = dataset_pd.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "# tokenize data set\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "texts = tokenizer.batch_encode_plus(dataset_pd['text'].tolist(),\n",
        "                                    padding=True, truncation=True,\n",
        "                                    return_tensors='tf')\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((dict(texts), dataset_pd['label']))\n",
        "\n",
        "# split training / validation\n",
        "batch_size = 32 * tpu_strategy.num_replicas_in_sync\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "val_data_size = int(0.1 * len(dataset_pd.index))\n",
        "train_data_size = len(dataset_pd.index) - val_data_size\n",
        "val_ds = dataset.take(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = dataset.skip(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "-qI7H3nSfFg5"
      },
      "id": "-qI7H3nSfFg5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Model"
      ],
      "metadata": {
        "id": "LaiHodlpfS8r"
      },
      "id": "LaiHodlpfS8r"
    },
    {
      "cell_type": "code",
      "source": [
        "USE_MODEL = \"read-var\" # alternatively \"read\""
      ],
      "metadata": {
        "id": "h28BjcsYiA8c"
      },
      "id": "h28BjcsYiA8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(learning_rate, use_model=\"basemodel\"):\n",
        "  \"\"\" loads the model and compiles it with the passed hyperparams.\n",
        "  Which model to use is chosen based on use_model.\n",
        "  returns a model ready to train.\n",
        "  \"\"\"\n",
        "\n",
        "  assert use_model == \"basemodel\" or use_model == \"read\" or use_model == \"read-var\", \"invalid model name, use 'basemodel', 'read' or 'read-var'\"\n",
        "\n",
        "  if use_model == \"basemodel\":\n",
        "    with tpu_strategy.scope():\n",
        "      model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=AutoConfig.from_pretrained(model_name))\n",
        "      model.compile(\n",
        "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "          optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=False, clipnorm=1.),\n",
        "          metrics=[tf.metrics.SparseCategoricalAccuracy()]\n",
        "      )\n",
        "\n",
        "  elif use_model == \"read\":\n",
        "    with tpu_strategy.scope():\n",
        "      model = GRUModel(model_name, 2, num_gru_units=8)\n",
        "      model.compile(\n",
        "          loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "          optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=False, clipnorm=None),\n",
        "          metrics=[tf.metrics.SparseCategoricalAccuracy()]\n",
        "      )\n",
        "\n",
        "  else:\n",
        "    with tpu_strategy.scope():\n",
        "      model = VGRUModel(model_name, 2, train_data_size=train_data_size, num_gru_units=8)\n",
        "      model.compile(\n",
        "          loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "          optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=False, clipnorm=None),\n",
        "          metrics=[tf.metrics.SparseCategoricalAccuracy()]\n",
        "      )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "m89ErPEtfWzo"
      },
      "id": "m89ErPEtfWzo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Search"
      ],
      "metadata": {
        "id": "uCc6bsMRjpdr"
      },
      "id": "uCc6bsMRjpdr"
    },
    {
      "cell_type": "code",
      "source": [
        "LARGEST_LR = 1e-3\n",
        "SMALLEST_LR = 1e-6\n",
        "NUM_POINTS = 16\n",
        "\n",
        "MAX_EPOCHS_PER_CONFIG = 1"
      ],
      "metadata": {
        "id": "VCkyuXcpaQjF"
      },
      "id": "VCkyuXcpaQjF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_range = 10**np.linspace(np.log10(SMALLEST_LR), np.log10(LARGEST_LR), NUM_POINTS)\n",
        "history_dict = {}\n",
        "\n",
        "for i in range(0, len(lr_range)):\n",
        "  lr = lr_range[i]\n",
        "  print(f\"HPS for lr = {lr:.2e}\")\n",
        "  model = get_model(lr, USE_MODEL)\n",
        "  history = model.fit(train_ds, validation_data=val_ds, epochs=MAX_EPOCHS_PER_CONFIG, verbose=1)\n",
        "\n",
        "  run_name = \"hps_\" + USE_MODEL + \"_lr=\" + f\"{lr:.2e}\"\n",
        "  mpath = run_name + \".h5\"\n",
        "\n",
        "  history_dict[run_name] = history\n",
        "  hd_name = run_name + \"_dict.pkl\"\n",
        "\n",
        "  model.save_weights(mpath)\n",
        "  with open(hd_name, 'wb') as f:\n",
        "      pickle.dump(history_dict, f)\n",
        "\n",
        "  !gs cp $mpath \"gs://cil_2023/models/\"\n",
        "  !gs cp $hd_name \"gs://cil_2023/models/\""
      ],
      "metadata": {
        "id": "cQe4Z1SyZHd0"
      },
      "id": "cQe4Z1SyZHd0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}