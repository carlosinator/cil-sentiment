{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Search hyperparameters for LLM"
      ],
      "metadata": {
        "id": "XmWajEN6eT4T"
      },
      "id": "XmWajEN6eT4T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Setup"
      ],
      "metadata": {
        "id": "XONmweYhecr1"
      },
      "id": "XONmweYhecr1"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "4G2RQFYQkp77"
      },
      "id": "4G2RQFYQkp77",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip3 install transformers emoji==0.6.0 keras_nlp"
      ],
      "metadata": {
        "id": "4kj6bSoKelL7",
        "outputId": "194104f6-2297-4e79-bcf3-05f982f60e8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4kj6bSoKelL7",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 57.9 MB/s eta 0:00:00\n",
            "Collecting emoji==0.6.0\n",
            "  Downloading emoji-0.6.0.tar.gz (51 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.0/51.0 kB 6.0 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.6.0-py3-none-any.whl (576 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 576.5/576.5 kB 46.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 27.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 103.5 MB/s eta 0:00:00\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 78.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Collecting keras-core (from keras_nlp)\n",
            "  Downloading keras_core-0.1.0-py3-none-any.whl (727 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 728.0/728.0 kB 62.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.4.2)\n",
            "Collecting tensorflow-text (from keras_nlp)\n",
            "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 106.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (2.12.0)\n",
            "Collecting namex (from keras-core->keras_nlp)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.14.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (0.13.0)\n",
            "Collecting tensorflow (from keras-core->keras_nlp)\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 524.1/524.1 MB 3.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.56.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow->keras-core->keras_nlp)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 84.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (16.0.0)\n",
            "Collecting numpy>=1.17 (from transformers)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 66.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow->keras-core->keras_nlp)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 114.4 MB/s eta 0:00:00\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow->keras-core->keras_nlp)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 440.8/440.8 kB 40.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (2.3.0)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-core->keras_nlp) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-core->keras_nlp) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras-core->keras_nlp) (3.2.2)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py): started\n",
            "  Building wheel for emoji (setup.py): finished with status 'done'\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49720 sha256=0ab94878f7de6bae8144f6e2ae8f6e40c53ca3e148462b82966a668cea95b090\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/bd/d9/310c33c45a553798a714e27e3b8395d37128425442b8c78e07\n",
            "Successfully built emoji\n",
            "Installing collected packages: tokenizers, safetensors, namex, emoji, typing-extensions, tensorflow-estimator, numpy, keras, huggingface-hub, transformers, tensorboard, tensorflow, tensorflow-text, keras-core, keras_nlp\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.1\n",
            "    Uninstalling numpy-1.25.1:\n",
            "      Successfully uninstalled numpy-1.25.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed emoji-0.6.0 huggingface-hub-0.16.4 keras-2.13.1 keras-core-0.1.0 keras_nlp-0.6.0 namex-0.0.7 numpy-1.24.3 safetensors-0.3.1 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-text-2.13.0 tokenizers-0.13.3 transformers-4.30.2 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.7 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\n",
            "flax 0.6.11 requires jax>=0.4.2, but you have jax 0.3.25 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "orbax-checkpoint 0.2.6 requires jax>=0.4.9, but you have jax 0.3.25 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/carlosinator/cil-sentiment.git"
      ],
      "metadata": {
        "id": "UQuSJ6uggr1q",
        "outputId": "64b28b2c-518e-472d-c9af-f792b84595ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UQuSJ6uggr1q",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cil-sentiment'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 34 (delta 9), reused 18 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), 80.38 KiB | 2.51 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, TFAutoModel, AutoConfig, TFAutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# reproducibility\n",
        "transformers.set_seed(0) # sets the seed in random, numpy, and tf"
      ],
      "metadata": {
        "id": "aUf_eJZ8esrm",
        "outputId": "1d9dc51d-c302-4eab-c0e2-55c5f416bc3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aUf_eJZ8esrm",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./cil-sentiment/models\")\n",
        "from gru_models import GRUModel"
      ],
      "metadata": {
        "id": "2DKmxZChk3F2"
      },
      "id": "2DKmxZChk3F2",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp \"gs://cil_2023/train_pos_preprocessed.txt\" .\n",
        "!gsutil cp \"gs://cil_2023/train_neg_preprocessed.txt\" .\n",
        "\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "filename_train_pos = \"train_pos_preprocessed.txt\"\n",
        "filename_train_neg = \"train_neg_preprocessed.txt\"\n",
        "\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "dDHTZfUcezks",
        "outputId": "3525e56a-9df9-407d-aeff-10187dcd2f54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dDHTZfUcezks",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cil_2023/train_pos_preprocessed.txt...\n",
            "\\\n",
            "Operation completed over 1 objects/82.2 MiB.                                     \n",
            "Copying gs://cil_2023/train_neg_preprocessed.txt...\n",
            "|\n",
            "Operation completed over 1 objects/101.5 MiB.                                    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.75.150.10:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function initialize_tpu_system.<locals>._tpu_init_fn at 0x7fae8efaf2e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "old_pd = pd.read_fwf(\"train_pos_preprocessed.txt\", sep='\\n', header=None, names=['text'])\n",
        "new_pd = pd.read_fwf(filename_train_pos, sep='\\n', header=None, names=['text'])"
      ],
      "metadata": {
        "id": "07xs2fg2tR9o"
      },
      "id": "07xs2fg2tR9o",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_pd"
      ],
      "metadata": {
        "id": "y5-yBwvvvWcs"
      },
      "id": "y5-yBwvvvWcs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset\n",
        "dataset_pos_pd = pd.read_fwf(filename_train_pos, sep='\\n', header=None, names=['text'])\n",
        "dataset_neg_pd = pd.read_fwf(filename_train_neg, sep='\\n', header=None, names=['text'])\n",
        "dataset_pos_pd['label'] = 0\n",
        "dataset_neg_pd['label'] = 1\n",
        "dataset_pd = pd.concat([dataset_pos_pd, dataset_neg_pd])\n",
        "\n",
        "# shuffle\n",
        "dataset_pd = dataset_pd.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "# tokenize data set\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "texts = tokenizer.batch_encode_plus(dataset_pd['text'].tolist(),\n",
        "                                    padding=True, truncation=True,\n",
        "                                    return_tensors='tf')\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((dict(texts), dataset_pd['label']))\n",
        "\n",
        "# split training / validation\n",
        "batch_size = 32 * tpu_strategy.num_replicas_in_sync\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "val_data_size = int(0.1 * len(dataset_pd.index))\n",
        "train_data_size = len(dataset_pd.index) - val_data_size\n",
        "val_ds = dataset.take(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = dataset.skip(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "-qI7H3nSfFg5",
        "outputId": "25b8f60e-4467-496b-f159-245763af1b26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-qI7H3nSfFg5",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Model"
      ],
      "metadata": {
        "id": "LaiHodlpfS8r"
      },
      "id": "LaiHodlpfS8r"
    },
    {
      "cell_type": "code",
      "source": [
        "USE_MODEL = \"basemodel\" # alternatively \"read\""
      ],
      "metadata": {
        "id": "h28BjcsYiA8c"
      },
      "id": "h28BjcsYiA8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(learning_rate, use_model=\"basemodel\"):\n",
        "  \"\"\" loads the model and compiles it with the passed hyperparams.\n",
        "  Which model to use is chosen based on use_model.\n",
        "  returns a model ready to train.\n",
        "  \"\"\"\n",
        "\n",
        "  assert use_model == \"basemodel\" or use_model == \"read\", \"invalid model name, use 'basemodel' or 'read'\"\n",
        "\n",
        "  if use_model == \"basemodel\":\n",
        "    with tpu_strategy.scope():\n",
        "      model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=AutoConfig.from_pretrained(model_name))\n",
        "      model.compile(\n",
        "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "          optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=False, clipnorm=1.),\n",
        "          metrics=[tf.metrics.SparseCategoricalAccuracy()]\n",
        "      )\n",
        "\n",
        "  else:\n",
        "    with tpu_strategy.scope():\n",
        "      model = GRUModel(model_name, 2, num_gru_units=8)\n",
        "      model.compile(\n",
        "          loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "          optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=False, clipnorm=None),\n",
        "          metrics=[tf.metrics.SparseCategoricalAccuracy()]\n",
        "      )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "m89ErPEtfWzo"
      },
      "id": "m89ErPEtfWzo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Search"
      ],
      "metadata": {
        "id": "uCc6bsMRjpdr"
      },
      "id": "uCc6bsMRjpdr"
    },
    {
      "cell_type": "code",
      "source": [
        "LARGEST_LR = 1e-3\n",
        "SMALLEST_LR = 1e-6\n",
        "NUM_POINTS = 1\n",
        "\n",
        "MAX_EPOCHS_PER_CONFIG = 1"
      ],
      "metadata": {
        "id": "FuSL-B80gvIz"
      },
      "id": "FuSL-B80gvIz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_range = 10**np.linspace(np.log10(SMALLEST_LR), np.log10(LARGEST_LR), NUM_POINTS)\n",
        "history_dict = {}\n",
        "\n",
        "for lr in lr_range:\n",
        "  model = get_model(lr, USE_MODEL)\n",
        "  history = model.fit(train_ds, validation_data=val_ds, epochs=MAX_EPOCHS_PER_CONFIG, verbose=1)\n",
        "\n",
        "  model_name = \"hps_\" + USE_MODEL + \"_lr=\" + f\"{lr:.2e}\"\n",
        "  mpath = model_name + \".h5\"\n",
        "\n",
        "  history_dict[model_name] = history\n",
        "\n",
        "  model.save_weights(mpath)\n",
        "  with open('history_dict.pkl', 'wb') as f:\n",
        "      pickle.dump(history_dict, f)\n",
        "\n",
        "  !gs cp $mpath \"gs://cil_2023/models/\"\n",
        "  !gs cp $'history_dict.pkl' \"gs://cil_2023/models/\""
      ],
      "metadata": {
        "id": "-rSW4pRNmOHQ"
      },
      "id": "-rSW4pRNmOHQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}