{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMys7SZa4SeBpw8v5sih/zp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosinator/cil-sentiment/blob/main/calibration_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Setup"
      ],
      "metadata": {
        "id": "XONmweYhecr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "4G2RQFYQkp77"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip3 install transformers emoji==0.6.0 keras_nlp"
      ],
      "metadata": {
        "id": "4kj6bSoKelL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/carlosinator/cil-sentiment.git"
      ],
      "metadata": {
        "id": "UQuSJ6uggr1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, TFAutoModel, AutoConfig, TFAutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import re\n",
        "import subprocess as sp\n",
        "import os\n",
        "from threading import Thread , Timer\n",
        "import sched, time\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"./cil-sentiment/models\")\n",
        "sys.path.append(\"./cil-sentiment/\")\n",
        "from gru_models import GRUModel, VGRUModel\n",
        "import utils\n",
        "\n",
        "# reproducibility\n",
        "transformers.set_seed(0) # sets the seed in random, numpy, and tf"
      ],
      "metadata": {
        "id": "aUf_eJZ8esrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp \"gs://cil_2023/train_pos_full_preprocessed_without_duplicates.txt\" .\n",
        "!gsutil cp \"gs://cil_2023/train_neg_full_preprocessed_without_duplicates.txt\" .\n",
        "\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "filename_train_pos = \"train_pos_full_preprocessed_without_duplicates.txt\"\n",
        "filename_train_neg = \"train_neg_full_preprocessed_without_duplicates.txt\"\n",
        "\n",
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "# tf.config.experimental_connect_to_cluster(tpu)\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "dDHTZfUcezks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset\n",
        "dataset_pos_pd = pd.read_table(filename_train_pos, sep='\\r\\n', header=None, names=['text'])\n",
        "dataset_neg_pd = pd.read_table(filename_train_neg, sep='\\r\\n', header=None, names=['text'])\n",
        "dataset_pos_pd['label'] = 0\n",
        "dataset_neg_pd['label'] = 1\n",
        "dataset_pd = pd.concat([dataset_pos_pd, dataset_neg_pd])\n",
        "\n",
        "# shuffle\n",
        "dataset_pd = dataset_pd.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "# tokenize data set\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "texts = tokenizer.batch_encode_plus(dataset_pd['text'].tolist(),\n",
        "                                    padding=True, truncation=True,\n",
        "                                    return_tensors='tf')\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((dict(texts), dataset_pd['label']))\n",
        "\n",
        "# split training / validation\n",
        "batch_size = 1024 # * tpu_strategy.num_replicas_in_sync\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "val_data_size = int(0.1 * len(dataset_pd.index))\n",
        "test_data_size = int(0.1 * len(dataset_pd.index))\n",
        "train_data_size = len(dataset_pd.index) - val_data_size - test_data_size\n",
        "val_ds = dataset.take(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "test_ds = dataset.take(test_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = dataset.skip(val_data_size).batch(batch_size, drop_remainder=True)\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "-qI7H3nSfFg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in Model"
      ],
      "metadata": {
        "id": "C6NbJNpbxkze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_MODEL = \"basemodel\" # alternatively 'read' or 'read-var'\n",
        "\n",
        "run_name = \"inference_\" + USE_MODEL + \"_fullmodel\"\n",
        "!gsutil cp -r {\"gs://cil_2023/models/\" + run_name} .\n",
        "\n",
        "model = tf.keras.models.load_model(run_name)"
      ],
      "metadata": {
        "id": "d8a3vdKhxmES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calibration"
      ],
      "metadata": {
        "id": "dU8frPjl4qr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.concatenate([y for x, y in test_ds])\n",
        "preds = model.predict(test_ds)"
      ],
      "metadata": {
        "id": "9JLSNu5L4sgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_BUCKETS = 20\n",
        "positives = 1 - labels\n",
        "confidences = preds[:,0]\n",
        "\n",
        "\n",
        "corrects = []\n",
        "conf_buckets = []\n",
        "\n",
        "for i in range(NUM_BUCKETS):\n",
        "  mask = np.logical_and(confidences >= i/NUM_BUCKETS, confidences < (i+1)/NUM_BUCKETS)\n",
        "  conf_buckets.append(confidences[mask])\n",
        "  corrects.append(labels[mask])"
      ],
      "metadata": {
        "id": "gqUZJi00qks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-bMN0J5psArh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}